{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Edge Computing with Raspberry Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Coral Edge TPU\n",
    "* 구글에서 설계한 소형 ASIC(주문형 반도체)\n",
    "* 저전력 정차의 고성능 머신러닝 추론 기능 제공\n",
    "* Tensorflow Lite only\n",
    "* 비전 기반 머신러닝 어플리케이션에 적합\n",
    "* Backpropagration을 하지 못하기 때문에 딥러닝 훈련에 사용 불능\n",
    "    * Transfer Learning 가능\n",
    "* Edge TPU에 사용할 모델은 TF Lite에 맞게 번환해야 하고 양자화 필요\n",
    "* 2가지 종류\n",
    "    * Dev Board : SoC와 Edge TPU를 통합한 싱글보드 컴퓨터\n",
    "    * USB Accelerator : USB 방식의 엑세서리 장치\n",
    "    * https://coral.withgoogle.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 설치\n",
    "### 요구사항\n",
    "* OS : Debian 6.0 이상 계열의 리눅스\n",
    "* CPU : x86-64 또는 ARM32/64 ARMv8 명령셋\n",
    "* Raspberry Pi 2/3 Model B/B+ 지원, Pi Zero는 비공식 지원\n",
    "* Edge TPU Runtime and Python API 설치\n",
    "\n",
    "### Edget TPU Runtime & Python Librar 설치\n",
    "* `cd ~/`\n",
    "* `wget https://dl.google.com/coral/edgetpu_api/edgetpu_api_latest.tar.gz -O edgetpu_api.tar.gz --trust-server-names `\n",
    "* `tar xzf edgetpu_api.tar.gz `\n",
    "* `cd edgetpu_api`\n",
    "* `bash ./install.sh `\n",
    "    * > maximum operation frequency?\n",
    "        * Y : Inference 속도를 최대로 끌어 올린다. USB 장치가 매우 뜨거워지니 화상에 주의 할것!\n",
    "        * N(Default) : 최대 속도 사용 안함\n",
    "        * 변경하려면 install.sh을 다시 실행\n",
    "    * 종속 라이브러리 설치, /lib/arm-linux-gnueabihf/libedgetpu.so 설치\n",
    "* USB 장치 연결(만약 미리 연결했다면 다시 연결)\n",
    "    * 전원 확인 : 장치 뒷면 LED(백색) 점등 확인\n",
    "        * `lsusb` :\n",
    "            * Bus 001 Device 004: ID 1a6e:089a Global Unichip Corp\n",
    "    * 동작 중 : 장치 뒷면 LED 점멸\n",
    "        * `lsusb` :\n",
    "            * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Demo \n",
    "### Demo 소스 코드\n",
    "* `cd /usr/local/lib/python3.5/dist-packages/edgetpu/demo`\n",
    "   * classify_image.py\n",
    "   * classify_capture.py\n",
    "   * object_detection.py\n",
    "   * classification_transfer_learning.py\n",
    "   * two_models_inference.py\n",
    "\n",
    "### Pre-Comiled Model\n",
    "* https://coral.withgoogle.com/models\n",
    "* Image classification\n",
    "    * MobileNet V1(ImageNet)\n",
    "    * MobileNet V2(ImageNet)\n",
    "    * MobileNet V2(iNat insercts)\n",
    "    * MobileNet V2(iNat plants)\n",
    "    * MobileNet V2(iNat birds)\n",
    "    * Inception V1(ImageNet)\n",
    "    * Inception V2(ImageNet)\n",
    "    * Inception V3(ImageNet)\n",
    "    * Inception V4(ImageNet)\n",
    "* Object Detection\n",
    "    * MobileNet SSD v1(COCO)\n",
    "    * MobileNet SSD v2(COCO)\n",
    "    * MobileNet SSD v2(Faces)\n",
    "* Embedding Extractor\n",
    "    * MobileNet v1\n",
    "* 원하는 모델과 Label(text) 파일을 다운로드\n",
    "\n",
    "### Image Classification\n",
    "* Model & Label Download\n",
    "    * `cd ~/Downloads`\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\n",
    "-O https://dl.google.com/coral/canned_models/inat_bird_labels.txt \\\n",
    "-O https://coral.withgoogle.com/static/images/parrot.jpg`\n",
    "* 실행\n",
    "    * `cd /usr/local/lib/python3.5/dist-packages/edgetpu/demo`\n",
    "    * `python3 classify_image.py \\\n",
    "--model ~/Downloads/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\n",
    "--label ~/Downloads/inat_bird_labels.txt \\\n",
    "--image ~/Downloads/parrot.jpg`\n",
    "* 결과\n",
    "    * `---------------------------\n",
    "Ara macao (Scarlet Macaw)\n",
    "Score :  0.761719`\n",
    "\n",
    "### Object Detection\n",
    "* Model & Label Download\n",
    "    * `cd ~/Downloads`\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite \\\n",
    "-O https://coral.withgoogle.com/static/images/face.jpg`\n",
    "* feh 이미지 뷰어 설치\n",
    "    * ` sudo apt-get install feh -y`\n",
    "* 데모 실행\n",
    "    * VNC 연결\n",
    "    * `python3 object_detection.py \\\n",
    "--model ~/Downloads/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite \\\n",
    "--input ~/Downloads/face.jpg \\\n",
    "--output ~/detection_results.jpg`\n",
    "\n",
    "* 실행 결과    \n",
    "![detection_result](https://coral.withgoogle.com/static/images/detection_results.jpg)\n",
    "\n",
    "### Realtime Camera classification\n",
    "* Model download\n",
    "    * `cd ~/Downloads`\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/mobilenet_v2_1.0_224_quant_edgetpu.tflite \\\n",
    "-O https://dl.google.com/coral/canned_models/imagenet_labels.txt`\n",
    "* 데모 실행\n",
    "    * `cd /usr/local/lib/python3.5/dist-packages/edgetpu/demo`\n",
    "    * `python3 classify_capture.py \\\n",
    "--model ~/Downloads/mobilenet_v2_1.0_224_quant_edgetpu.tflite \\\n",
    "--label ~/Downloads/imagenet_labels.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge TPU Python API\n",
    "* edetpu.basic.basic_engine : https://coral.withgoogle.com/docs/reference/edgetpu.basic.basic_engine/\n",
    "    * BasicEngine\n",
    "    * Tensorflow Lite Model을 Edge TPU에서 실행하기 위한 기반 엔진\n",
    "* edgetpu.basic.edgetpu_utils : https://coral.withgoogle.com/docs/reference/edgetpu.basic.edgetpu_utils/\n",
    "    * Utility 함수와 상수\n",
    "* edgetpu.classification.engine : https://coral.withgoogle.com/docs/reference/edgetpu.classification.engine/\n",
    "    * ClassificationEngine\n",
    "    * 이미지 분류를 위한 BasicEngie 확장 클래스\n",
    "* edgetpu.detection.engine : https://coral.withgoogle.com/docs/reference/edgetpu.detection.engine/\n",
    "    * DetectionCandidate\n",
    "        * detection 후보 정보를 위한 구조체\n",
    "    * DetectionEngine\n",
    "        * Object Detection을 위한 BasicEngine 확장 클래스\n",
    "* edgetpu.learn.imprinting.engine : https://coral.withgoogle.com/docs/reference/edgetpu.learn.imprinting.engine/\n",
    "    * ImprintingEngine\n",
    "    * 이미지 분류 transfer-learning을 위한 엔진\n",
    "* edgetpu.utils.image_process : https://coral.withgoogle.com/docs/reference/edgetpu.utils.image_processing/\n",
    "    * ResamplingWithOriginalRatio\n",
    "    * 이미지 전처리를 위한 유틸릴티\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Object Detection 예제\n",
    "### MobileNet SSD V2(Coco) Sync\n",
    "* 모델 다운로드\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite`\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/coco_labels.txt`\n",
    "* 실행  \n",
    "    * `python3 cam_detect_sync.py`\n",
    "* 소스 코드\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import platform\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "from edgetpu.detection.engine import DetectionEngine\n",
    "\n",
    "\n",
    "# Function to read labels from text files.\n",
    "def ReadLabelFile(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "  ret = {}\n",
    "  for line in lines:\n",
    "    pair = line.strip().split(maxsplit=1)\n",
    "    ret[int(pair[0])] = pair[1].strip()\n",
    "  return ret\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", default=\"mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite\", help=\"Path of the detection model.\")\n",
    "    parser.add_argument(\"--label\", default=\"coco_labels.txt\", help=\"Path of the labels file.\")\n",
    "    parser.add_argument(\"--usbcamno\", type=int, default=0, help=\"USB Camera number.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    fps = \"\"\n",
    "    detectfps = \"\"\n",
    "    framecount = 0\n",
    "    detectframecount = 0\n",
    "    time1 = 0\n",
    "    time2 = 0\n",
    "    box_color = (255, 128, 0)\n",
    "    box_thickness = 1\n",
    "    label_background_color = (125, 175, 75)\n",
    "    label_text_color = (255, 255, 255)\n",
    "    percentage = 0.0\n",
    "\n",
    "    camera_width = 320\n",
    "    camera_height = 240\n",
    "\n",
    "    cap = cv2.VideoCapture(args.usbcamno)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 150)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n",
    "\n",
    "    # Initialize engine.\n",
    "    engine = DetectionEngine(args.model)\n",
    "    labels = ReadLabelFile(args.label) if args.label else None\n",
    "\n",
    "    while True:\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        ret, color_image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Run inference.\n",
    "        prepimg = color_image[:, :, ::-1].copy()\n",
    "        prepimg = Image.fromarray(prepimg)\n",
    "\n",
    "        tinf = time.perf_counter()\n",
    "        ans = engine.DetectWithImage(prepimg, threshold=0.5, keep_aspect_ratio=True, relative_coord=False, top_k=10)\n",
    "        print(time.perf_counter() - tinf, \"sec\")\n",
    "\n",
    "\n",
    "        # Display result.\n",
    "        if ans:\n",
    "            detectframecount += 1\n",
    "            for obj in ans:\n",
    "                box = obj.bounding_box.flatten().tolist()\n",
    "                box_left = int(box[0])\n",
    "                box_top = int(box[1])\n",
    "                box_right = int(box[2])\n",
    "                box_bottom = int(box[3])\n",
    "                cv2.rectangle(color_image, (box_left, box_top), (box_right, box_bottom), box_color, box_thickness)\n",
    "\n",
    "                percentage = int(obj.score * 100)\n",
    "                label_text = labels[obj.label_id] + \" (\" + str(percentage) + \"%)\" \n",
    "\n",
    "                label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                label_left = box_left\n",
    "                label_top = box_top - label_size[1]\n",
    "                if (label_top < 1):\n",
    "                    label_top = 1\n",
    "                label_right = label_left + label_size[0]\n",
    "                label_bottom = label_top + label_size[1]\n",
    "                cv2.rectangle(color_image, (label_left - 1, label_top - 1), (label_right + 1, label_bottom + 1), label_background_color, -1)\n",
    "                cv2.putText(color_image, label_text, (label_left, label_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_text_color, 1)\n",
    "\n",
    "        cv2.putText(color_image, fps,       (camera_width-170,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(color_image, detectfps, (camera_width-170,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.namedWindow('USB Camera', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('USB Camera', color_image)\n",
    "\n",
    "        if cv2.waitKey(1)&0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        # FPS calculation\n",
    "        framecount += 1\n",
    "        if framecount >= 15:\n",
    "            fps       = \"(Playback) {:.1f} FPS\".format(time1/15)\n",
    "            detectfps = \"(Detection) {:.1f} FPS\".format(detectframecount/time2)\n",
    "            framecount = 0\n",
    "            detectframecount = 0\n",
    "            time1 = 0\n",
    "            time2 = 0\n",
    "        t2 = time.perf_counter()\n",
    "        elapsedTime = t2-t1\n",
    "        time1 += 1/elapsedTime\n",
    "        time2 += elapsedTime\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MobileNet SSD V2(Coco) ASync\n",
    "* 쓰레드로 속도 개선\n",
    "* 모델 다운로드\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite`\n",
    "    * `curl -O https://dl.google.com/coral/canned_models/coco_labels.txt`\n",
    "* 실행  \n",
    "    * `python3 cam_detect_async.py`\n",
    "* 소스 코드\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import platform\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "import multiprocessing as mp\n",
    "from edgetpu.detection.engine import DetectionEngine\n",
    "\n",
    "lastresults = None\n",
    "processes = []\n",
    "frameBuffer = None\n",
    "results = None\n",
    "fps = \"\"\n",
    "detectfps = \"\"\n",
    "framecount = 0\n",
    "detectframecount = 0\n",
    "time1 = 0\n",
    "time2 = 0\n",
    "box_color = (255, 128, 0)\n",
    "box_thickness = 1\n",
    "label_background_color = (125, 175, 75)\n",
    "label_text_color = (255, 255, 255)\n",
    "percentage = 0.0\n",
    "\n",
    "# Function to read labels from text files.\n",
    "def ReadLabelFile(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "  ret = {}\n",
    "  for line in lines:\n",
    "    pair = line.strip().split(maxsplit=1)\n",
    "    ret[int(pair[0])] = pair[1].strip()\n",
    "  return ret\n",
    "\n",
    "\n",
    "def camThread(label, results, frameBuffer, camera_width, camera_height, vidfps, usbcamno):\n",
    "\n",
    "    global fps\n",
    "    global detectfps\n",
    "    global framecount\n",
    "    global detectframecount\n",
    "    global time1\n",
    "    global time2\n",
    "    global lastresults\n",
    "    global cam\n",
    "    global window_name\n",
    "\n",
    "    cam = cv2.VideoCapture(usbcamno)\n",
    "    cam.set(cv2.CAP_PROP_FPS, vidfps)\n",
    "    cam.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n",
    "    cam.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n",
    "    window_name = \"USB Camera\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while True:\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        ret, color_image = cam.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        if frameBuffer.full():\n",
    "            frameBuffer.get()\n",
    "        frames = color_image\n",
    "        frameBuffer.put(color_image.copy())\n",
    "        res = None\n",
    "\n",
    "        if not results.empty():\n",
    "            res = results.get(False)\n",
    "            detectframecount += 1\n",
    "            imdraw = overlay_on_image(frames, res, label, camera_width, camera_height)\n",
    "            lastresults = res\n",
    "        else:\n",
    "            imdraw = overlay_on_image(frames, lastresults, label, camera_width, camera_height)\n",
    "\n",
    "        cv2.imshow('USB Camera', imdraw)\n",
    "\n",
    "        if cv2.waitKey(1)&0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        # FPS calculation\n",
    "        framecount += 1\n",
    "        if framecount >= 15:\n",
    "            fps       = \"(Playback) {:.1f} FPS\".format(time1/15)\n",
    "            detectfps = \"(Detection) {:.1f} FPS\".format(detectframecount/time2)\n",
    "            framecount = 0\n",
    "            detectframecount = 0\n",
    "            time1 = 0\n",
    "            time2 = 0\n",
    "        t2 = time.perf_counter()\n",
    "        elapsedTime = t2-t1\n",
    "        time1 += 1/elapsedTime\n",
    "        time2 += elapsedTime\n",
    "\n",
    "\n",
    "\n",
    "def inferencer(results, frameBuffer, model, camera_width, camera_height):\n",
    "\n",
    "    engine = DetectionEngine(model)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if frameBuffer.empty():\n",
    "            continue\n",
    "\n",
    "        # Run inference.\n",
    "        color_image = frameBuffer.get()\n",
    "        prepimg = color_image[:, :, ::-1].copy()\n",
    "        prepimg = Image.fromarray(prepimg)\n",
    "\n",
    "        tinf = time.perf_counter()\n",
    "        ans = engine.DetectWithImage(prepimg, threshold=0.5, keep_aspect_ratio=True, relative_coord=False, top_k=10)\n",
    "        print(time.perf_counter() - tinf, \"sec\")\n",
    "        results.put(ans)\n",
    "\n",
    "\n",
    "\n",
    "def overlay_on_image(frames, object_infos, label, camera_width, camera_height):\n",
    "\n",
    "    color_image = frames\n",
    "\n",
    "    if isinstance(object_infos, type(None)):\n",
    "        return color_image\n",
    "    img_cp = color_image.copy()\n",
    "\n",
    "    for obj in object_infos:\n",
    "        box = obj.bounding_box.flatten().tolist()\n",
    "        box_left = int(box[0])\n",
    "        box_top = int(box[1])\n",
    "        box_right = int(box[2])\n",
    "        box_bottom = int(box[3])\n",
    "        cv2.rectangle(img_cp, (box_left, box_top), (box_right, box_bottom), box_color, box_thickness)\n",
    "\n",
    "        percentage = int(obj.score * 100)\n",
    "        label_text = label[obj.label_id] + \" (\" + str(percentage) + \"%)\" \n",
    "\n",
    "        label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "        label_left = box_left\n",
    "        label_top = box_top - label_size[1]\n",
    "        if (label_top < 1):\n",
    "            label_top = 1\n",
    "        label_right = label_left + label_size[0]\n",
    "        label_bottom = label_top + label_size[1]\n",
    "        cv2.rectangle(img_cp, (label_left - 1, label_top - 1), (label_right + 1, label_bottom + 1), label_background_color, -1)\n",
    "        cv2.putText(img_cp, label_text, (label_left, label_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_text_color, 1)\n",
    "\n",
    "    cv2.putText(img_cp, fps,       (camera_width-170,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(img_cp, detectfps, (camera_width-170,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "    return img_cp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", default=\"mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite\", help=\"Path of the detection model.\")\n",
    "    parser.add_argument(\"--label\", default=\"coco_labels.txt\", help=\"Path of the labels file.\")\n",
    "    parser.add_argument(\"--usbcamno\", type=int, default=0, help=\"USB Camera number.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model    = args.model\n",
    "    label    = ReadLabelFile(args.label)\n",
    "    usbcamno = args.usbcamno\n",
    "\n",
    "    camera_width = 320\n",
    "    camera_height = 240\n",
    "    vidfps = 30\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method('forkserver')\n",
    "        frameBuffer = mp.Queue(10)\n",
    "        results = mp.Queue()\n",
    "\n",
    "        # Start streaming\n",
    "        p = mp.Process(target=camThread,\n",
    "                       args=(label, results, frameBuffer, camera_width, camera_height, vidfps, usbcamno),\n",
    "                       daemon=True)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "        # Activation of inferencer\n",
    "        p = mp.Process(target=inferencer,\n",
    "                       args=(results, frameBuffer, model, camera_width, camera_height),\n",
    "                       daemon=True)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "        while True:\n",
    "            sleep(1)\n",
    "\n",
    "    finally:\n",
    "        for p in range(len(processes)):\n",
    "            processes[p].terminate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow on Edge TPU\n",
    "\n",
    "* https://coral.withgoogle.com/docs/edgetpu/models-intro/\n",
    "* Edge TPU에 호환되는 모델 만드는 방법에 대해 설명한다.\n",
    "* Quantization(양자화) : Edge TPU에 필수요건\n",
    "    * 32bit float를 8bit fixed-point로 변경\n",
    "    * 모델을 작고 빠르게 한다. 모델 표현의 정확도는 떨어 질 수 있지만, 신경망의 추론에는 영향이 별로 없다. \n",
    "*  전체 흐름(아래 그림)\n",
    "![edge tpu](https://coral.withgoogle.com/static/images/compile-workflow.png)\n",
    "\n",
    "### 모델 컴파일\n",
    "* TF-Lite로 convert 한 모델을 컴파일 해야 Edge TPU에서 돌아 간다.\n",
    "* https://coral.withgoogle.com/web-compiler/\n",
    "    * TF-Lite로 convert 한 모델을 업로드해서 컴파일 진행\n",
    "* 컴파일 요구 사항\n",
    "    * 텐서 파라미터 양자화(8비트 고정 소수점 숫자), 아래 링크 이용\n",
    "        * https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize#quantization-aware-training\n",
    "        * post-training quantization 지원 안됨( https://www.tensorflow.org/lite/performance/post_training_quantization)\n",
    "            * 8bit 고정길이로 줄인 모델도 inference 하는 동안 32bit 부동소수점이 되기 때문이다.\n",
    "    * 텐서 사이즈는 컴파일 타임에 고정(다이나믹 사이즈 불가)\n",
    "    * 바이어스 같은 모델 파라미터는 상수화, 컴파일 타임에\n",
    "    * 텐서는 1,2,3차원 중에 하나만 가능\n",
    "    * 모델은 Edge TPU에서 지원하는 연산만 사용\n",
    "        * https://coral.withgoogle.com/docs/edgetpu/models-intro/#ops-table\n",
    "* 만약 요구 사항을 만족하지 못하면 컴파일은 되지만 실행될때 일부분만 Edge TPU에서 실행되고 그렇지 않은 부분은 CPU에서 실행된다.\n",
    "    * 모델은 1회만 구분할 수 있어서 미지원 연산을 사용한 이후 모든 부분은 CPU에서 실행된다.\n",
    "    * visualize.py 같은 도구로 조사해 볼 수 있다.\n",
    "        * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py\n",
    "\n",
    "### Transfer learning\n",
    "* 전체 흐름을 다 따를 필요없이 이미 존재하는 Edge TPU 호환 모델을  재훈련해서 사용 가능\n",
    "* fine tuning 이라고도 부름\n",
    "* 모델 전체의 weights와 bias를 재 훈련 할 수 도 있지만,\n",
    "* classification을 실행하는 마지막 레이어만 제거하고 새로운 클래스를 인식하는 새로운 레이러만 훈련해서도 할 수 있다.\n",
    "* 이미 훈련된 MobileNet에 새로운 클래스를 추가하는 튜토리얼이 있다.\n",
    "* Edge TPU에서 재훈련도 가능하다.\n",
    "   \n",
    "### Retrain an Image Classification on Device\n",
    "* 장점\n",
    "    * 거의 실시간으로 transfer-learning이 엣지 장치에서 일어난다.\n",
    "    * 모델을 재컴파일 할 필요 없다.\n",
    "* 단점\n",
    "    * 클래스당 최대 200개 이미지로 훈련 데이타 사이즈가 제한된다.\n",
    "    * 작은 내부적 클래스 변화만 갖는 데이타셋 에만 의미가 있다.\n",
    "    * 최종 fully connected layer는 CPU에서 돌아 가기 때문에 pre-compile 된 모델을 돌리는 것 보다 효과가 덜하다.\n",
    "* 주요 방법\n",
    "    * ImprintingEngine API 사용\n",
    "    * classification_transfer_learning.py 예제 스크립트 제공\n",
    "* 데모 작업 순서\n",
    "    * 꽃 데이타셋 다운로드 및 압축해제\n",
    "        * `wget http://download.tensorflow.org/example_images/flower_photos.tgz`\n",
    "        * `tar -xvfz flower_photos.tgz`\n",
    "    * embedding extractor 다운로드\n",
    "        * `wget http://storage.googleapis.com/cloud-iot-edge-pretrained-models/canned_models/mobilenet_v1_1.0_224_quant_embedding_extractor_edgetpu.tflite`\n",
    "    * transfer learning 시작\n",
    "        * `python3 /usr/local/lib/python3.5/dist-packages/edgetpu/demo/classification_transfer_learning.py \\\n",
    "        --extractor mobilenet_v1_1.0_224_quant_embedding_extractor_edgetpu.tflite \\\n",
    "        --data flower_photos \\\n",
    "        --output flower_model.tflite --test_ratio 0.95`\n",
    "        * 1~2분 소요 후에 flower_model.tflite 파일이 생성된다.\n",
    "* 데모 실행\n",
    "    * `wget -O rose.jpg https://c2.staticflickr.com/4/3062/3067374593_f2963e50b7_o.jpg`\n",
    "    * `python3 /usr/local/lib/python3.5/dist-packages/edgetpu/demo/classify_image.py \\\n",
    "     --model flower_model.tflite \\\n",
    "     --label flower_model.txt \\\n",
    "     --image rose.jpg`\n",
    "* 주의\n",
    "    * 반드시 재훈련할 모델의 embedding extractor version을 써야 한다.\n",
    "    * classification_transfer_learning.py 는 각 클래스별로 디렉토리를 만들어야 한다.\n",
    "    * API는 클래스별로 최대 200개의 훈련이미지만 지원한다.\n",
    "    * test_tatio flag로 훈련과 테스트 이미지 비율을 제어할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
