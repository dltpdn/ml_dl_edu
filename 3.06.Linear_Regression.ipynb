{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression 개념 정리\n",
    "* Linear Regression을 Numpy 만으로 구현해 본다.\n",
    "* 단순화 하기 위해서 계수 1개만으로 제한 한다.\n",
    "* $y = ax + b$\n",
    "* $H(x) = Wx + b$\n",
    "    * $H(x)$ : Hypothesis\n",
    "    * $W$ : Weight\n",
    "    * $b$ : bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data 만들기\n",
    "* m개의 데이타를 만든다.\n",
    "* X 데이타 : np.arange(m)\n",
    "* y 데이타 : X * 2, 약간의 노이즈를 위해 랜덤한 수에 10을 곱해서 추가한다.\n",
    "    * `np.random.randn(m)`은 평균이 0이고 표준편차가 1인 난수를 발생한다.\n",
    "* 데이타를 점으로 표시하고 정답 모델을 선으로 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "m = 50 #sample count\n",
    "X = np.arange(m)\n",
    "y = X  * 2 # W = 2, b=0 \n",
    "y = y + 10 * np.random.randn(m) # adding noise\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, X*2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis와 Cost함수\n",
    "* Hypothesis\n",
    "    * $H(W) = Wx^{(i)}$\n",
    "* Cost함수는 MSE를 사용한다.\n",
    "    * $\\text{cost}(W) = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m(H(W) - y^{(i)})^2$\n",
    "* W 값을 0.1 ~ 4.0까지 0.1씩 증가 하면서 cost를 구해서 시각화한다.\n",
    "    * $W$의 변화에 따라 Cost의 변화량을 시각화해 보면 MSE 함수는 Convex 함수임을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_range = np.arange(0.1, 4, 0.1) \n",
    "costs = np.array([])\n",
    "for i, w in enumerate(w_range):\n",
    "    h = w*X\n",
    "    cost = 1/m * np.sum( (h - y)**2)\n",
    "    costs = np.append(costs, cost)\n",
    "    #plt.plot(w, cost, 'r.')\n",
    "plt.plot(w_range, costs, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost 함수와 Gradient\n",
    "\n",
    "* Cost 함수가 Convex 함수이기 때문에 최소값을 찾으려면 기울기를 따라 내려가면 된다.\n",
    "* 미분을 하면 기울기를 얻을 수 있다.\n",
    "    * $\\displaystyle\\frac{\\partial}{\\partial W}cost(W) = \\frac{2}{m}\\sum_{i=1}^m(H(W) -y^{(i)})x^{(i)}$\n",
    "* 미분한 결과를 Cost 함수의 변화에 시각화하면 다음과 같다.\n",
    "* 미분한 결과를 일정한 값(learning_rate)의 비율로 W에서 빼는 것을 반복하다 보면 cost가 가장 작은 W를 찾을 수 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "plt.plot(w_range, costs, 'ro')\n",
    "for w, cost in zip(w_range, costs):\n",
    "    h = w*X\n",
    "    gradient =  2/m * np.sum( (h-y) * X)\n",
    "    #plt.plot(w_range, gradient *w_range + cost)\n",
    "    plt.plot(w_range, gradient*(w_range-w) +cost) #gradient는 x=0을 기준으로 계산되었기 때문에 \n",
    "    plt.axis([0, 4, 0, 3000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "* 전체 샘플에 대해서 경사 하강법을 이용해서 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "\n",
    "W = np.random.rand()\n",
    "plt.plot(X, W*X, 'b', label='Initial H(x)')\n",
    "print('initial W', W)\n",
    "\n",
    "learning_rate = 0.00005\n",
    "suspend = 0.00001\n",
    "old_g = None\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    hypothesis = W * X \n",
    "    loss = hypothesis - y\n",
    "    cost = 1/m * np.sum(loss**2)\n",
    "    gradient =  2/m * np.sum(loss * X)\n",
    "    if old_g is not None and abs(gradient - old_g) < suspend:\n",
    "        print('suspended:', epoch)\n",
    "        break\n",
    "    old_g = gradient\n",
    "    W = W - learning_rate * gradient\n",
    "    if epoch %20 == 0:\n",
    "        print('epoch:',epoch,' cost:',cost,' gradient:',gradient, ' W:', W)\n",
    "        plt.plot(X, W*X, '--')\n",
    "print('Final W:', W)\n",
    "plt.plot(X, W*X, 'r', label='Final H(x)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "* 모든 샘플에 대해서 비용을 계산하면 샘플수가 많을 수록 계산 속도가 오래 걸린다.\n",
    "* 각 샘플 중에 랜덤하게 한개만 선택해서 Gradient를 계산해서 속도를 개선한다.\n",
    "* 단점은 선택한 샘플에 따라 수렴율이 일정하지 않아서 최적치를 찾지 못할 수 있다.\n",
    "    * 학습률을 처음에는 크게 했다가 점진적으로 감소시켜야 한다.\n",
    "    * 매 반복에서 학습률을 결정하는 함수를 학습 스케쥴(learning schedule)이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "w_range = np.arange(0, 4, learning_rate)\n",
    "for i, w in enumerate(w_range):\n",
    "    random_i = np.random.randint(m)\n",
    "    xi = X[random_i]\n",
    "    yi = y[random_i]\n",
    "    h = w*xi\n",
    "    sgd_cost = (h - yi)**2\n",
    "    plt.plot(w, sgd_cost, 'ro')\n",
    "    sgd_gradient =  2 *(h-yi) * xi\n",
    "    plt.plot(w_range, sgd_gradient*(w_range-w) +sgd_cost) #gradient는 x=0을 기준으로 계산되었기 때문에 \n",
    "    plt.axis([0, 4, -100, 3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "\n",
    "W = np.random.rand()\n",
    "plt.plot(X, W*X, 'b', label='Initial H(x)')\n",
    "print('initial W', W)\n",
    "\n",
    "t0, t1 = 1, 100\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for i in range(m):\n",
    "        random_i = np.random.randint(m)\n",
    "        xi = X[random_i]\n",
    "        yi = y[random_i]\n",
    "        h = W*xi\n",
    "        cost = (h-yi)**2\n",
    "        gradient = 2*(h-yi) * xi\n",
    "        learning_rate = t0/(t1 + (epoch * m + i*10))\n",
    "        W = W - learning_rate * gradient\n",
    "    if epoch %5 ==0:\n",
    "        print('epoch:',epoch, 'cost:', cost,' gradient:',gradient, ' W:', W, 'learn_rate:', learning_rate)\n",
    "        plt.plot(X, W*X, '--')\n",
    "print('Final W:', W)\n",
    "plt.plot(X, W*X, 'r', label='Final H(x)')\n",
    "plt.legend()\n",
    "plt.axis([0, 50, 0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "* Batch GD와 Stochastic GD를 혼합한 형태\n",
    "* 임의의 작은 샘플 셋트에 대해서 그레이디언트를 계산\n",
    "* SGD에 비해 덜 불규칙하고 더 정확한 최소값에 도달\n",
    "* 지역 최소값에 빠질 가능성 높음(Convex 함수가 아닌경우)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "\n",
    "W = np.random.rand()\n",
    "plt.plot(X, W*X, 'b', label='Initial H(x)')\n",
    "print('initial W', W)\n",
    "\n",
    "t0, t1 = 1, 100\n",
    "epochs = 50\n",
    "batches = 5\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(m/batches)):\n",
    "        random_i = np.random.randint(0,m, (batches,))\n",
    "        xi = X[random_i]\n",
    "        yi = y[random_i]\n",
    "        h = W*xi\n",
    "        cost = 1/batches* np.sum((h-yi)**2)\n",
    "        gradient = 2/batches* np.sum((h-yi) * xi)\n",
    "        learning_rate = t0/(t1 + (epoch * (m + i*10)))\n",
    "        #learning_rate = 0.0005\n",
    "        W = W - learning_rate * gradient\n",
    "    if epoch %5 ==0:\n",
    "        print('epoch:',epoch, 'cost:', cost,' gradient:',gradient, ' W:', W, 'learn_rate:', learning_rate)\n",
    "        plt.plot(X, W*X, '--')\n",
    "print('Final W:', W)\n",
    "plt.plot(X, W*X, 'r', label='Final H(x)')\n",
    "plt.legend()\n",
    "plt.axis([0, 50, 0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Scikit-learn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(-1,1), y.reshape(-1,1))\n",
    "W = lr.coef_\n",
    "b = lr.intercept_\n",
    "print('W:', W, 'b:', b)\n",
    "plt.plot(X, W.ravel()*X + b, 'r-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
