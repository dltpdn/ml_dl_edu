{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Classification\n",
    "* 여러 클래스를 분류하는 방법\n",
    "    * 이진 분류기를 여러번 쓰는 방법에 비해 행렬 연산을 하는 것이 효과적\n",
    "* 이진 분류기를 여러번 쓰는 것\n",
    "    * 클래스 A 모델:\n",
    "    $\\begin{bmatrix}x_1 & x_2 \\end{bmatrix}\n",
    "    \\begin{bmatrix}w_{a1} \\\\ w_{a2}\\end{bmatrix}\n",
    "    = \\begin{bmatrix}w_{a1}x_1 +w_{a2}x_2\\end{bmatrix}$\n",
    "    * 클래스 B 모델:\n",
    "    $\\begin{bmatrix}x_1 & x_2 \\end{bmatrix}\n",
    "    \\begin{bmatrix}w_{b1} \\\\ w_{b2}\\end{bmatrix}\n",
    "    =\\begin{bmatrix}w_{b1}x_1 +w_{b2}x_2\\end{bmatrix}$\n",
    "    * 클래스 C 모델:$\\begin{bmatrix}x_1 & x_2 \\end{bmatrix}\n",
    "    \\begin{bmatrix}w_{c1} \\\\ w_{c2}\\end{bmatrix}\n",
    "    =\\begin{bmatrix}w_{c1}x_1 +w_{c2}x_2\\end{bmatrix}$\n",
    "* 하나의 행렬로 계산\n",
    "    * $\\begin{bmatrix}x_1 & x_2 \\end{bmatrix}\n",
    "    \\begin{bmatrix}w_{a1}  & w_{b1} & w_{c1} \\\\ w_{a2} & w_{b2} &w_{c2}  \\end{bmatrix}\n",
    "    = \\begin{bmatrix}w_{a1}x_1 +w_{a2}x_2 & w_{b1}x_1 +w_{b2}x_2 & w_{c1}x_1 +w_{c2}x_2 \\end{bmatrix}$\n",
    "\n",
    "### SoftMax Regrerssion\n",
    "* 다항 로지스틱 휘귀\n",
    "* Sigmoid 대신 Softmax 함수 사용\n",
    "    * Softmax : 각 출력 값은 0~1 사이, 모든 클래스의 값의 합은 1\n",
    "    * sigmoid 보다 좋은 이유 : 하나의 클래스가 높은 값을 갖으면 나머지 클래스는 아주 작은 값, 배타적\n",
    "* $\\displaystyle \\hat{P_k} = \\frac{e^{z_i}}{e^{z_0} + e^{z_1} + e^{z_2}... + e^{z_k}}= \\frac{e^{z_i}}{\\sum_{j=0}^ke^{z_k}}$\n",
    "\n",
    "### Softmax Prediction\n",
    "* Softmax 결과에서 확률이 가장 높은 클래스 반환\n",
    "* $\\hat{y} = argmax(\\hat{p})$\n",
    "\n",
    "### Cross Entropy\n",
    "* 소프트 맥스의 비용함수 :\n",
    "$\\displaystyle J(\\theta) =  -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^ky_k^{(i)}log(\\hat{p_k})$\n",
    "* 소프트맥스를 통과한 결과값은 어떤 특정한 클래스만 높은 값을 갖고 나머지 클래스는 아주 작은 값을 갖는다.\n",
    "여기에 y는 정답 클래스인 경우만 1을 나머지는 0을 가지고 있으므로 소프트맥스를 통과한 결과와 y를 곱하는 것만으로도 정답만 값이 나오고 틀린 결과는 0에 가까울 수 밖에 없다. 그런데 소프트맥스를 통과한 값을 $log()$함수를 씌우면 그 반대의 결과가 나온다. 따라서 틀리면 큰값 맞으면 작은 값이 나오므로 비용함수로 적합하다.\n",
    "\n",
    "### Gradient of Cross Entropy\n",
    "* $\\displaystyle \\nabla J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m(\\hat{p}_k^{(i)} - y_k^{(i)})x^{(i)}$\n",
    "\n",
    "* 참고 강좌\n",
    "    * 김성훈 교수(https://www.youtube.com/watch?v=MFAnsx1y9ZI&feature=youtu.be)\n",
    "    * 정리 : http://saitcelebi.com/tut/output/part2.html\n",
    "    * http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/\n",
    "    * https://deepnotes.io/softmax-crossentropy\n",
    "    * 크로스 엔트로피의 확률통계학적 : https://www.slideshare.net/jaepilko10/ss-91071277\n",
    "    * 크로스 엔트로피 정보학 : https://www.youtube.com/watch?v=ErfnhcEV1O8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.1, 1.4],\n",
    "              [-0.5, 0.2],\n",
    "              [ 1.3, 0.9],\n",
    "              [-0.6, 0.4],\n",
    "              [-1.6, 0.2],\n",
    "              [ 0.2, 0.2],\n",
    "              [-0.3,-0.4],\n",
    "              [ 0.7,-0.8],\n",
    "              [ 1.1,-1.5],\n",
    "              [-1.0, 0.9],\n",
    "              [-0.5, 1.5],\n",
    "              [-1.3,-0.4],\n",
    "              [-1.4,-1.2],\n",
    "              [-0.9,-0.7],\n",
    "              [ 0.4,-1.3],\n",
    "              [-0.4, 0.6],\n",
    "              [ 0.3,-0.5],\n",
    "              [-1.6,-0.7],\n",
    "              [-0.5,-1.4],\n",
    "              [-1.0,-1.4]])\n",
    "print(X[:5], X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 0, 2, 2, 2, 1, 0, 1, 2, 2, 2])\n",
    "y_enc = np.eye(3)[y]\n",
    "print('one-hot encoding:\\n', y_enc[:5], y_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.random.rand(2,3)\n",
    "bias = np.random.rand(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_input(X, W):\n",
    "    return (X.dot(W)) \n",
    "net_in = net_input(X, W)\n",
    "print('net input:\\n', X, net_in, net_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exps = np.exp(z - np.max(z)) #exps = np.ex(z)를 정규화\n",
    "    return exps / np.sum(exps, axis=1).reshape(-1,1)\n",
    "\n",
    "smax = softmax(net_in)\n",
    "print('softmax:\\n', smax, smax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis=1)\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크로스 엔트로피가 각 클래스별 값을 갖어야 하나 단 하나의 값을 가져야 하나 고민을 했었다.\n",
    "#크로스 엔트로피는 소프트맥스에 로그를해서 나온 결과를 y와 곱한 결과로 결국 하나의 샘플에 하나의 값만을 가질 수 밖에 없다.\n",
    "#따라서 클래스별로 값을 갖는 다는 것은 말이 안된다.\n",
    "#모든 샘플에 대해서 비용을 계산하기 위해 최종적으로 각 샘플별 크로스 엔트로피 결과를 평균내어 단 하나의 값을 가져야 한다.\n",
    "def cross_entropy(output, y_target):\n",
    "    s=  np.sum(np.log(output) * (y_target), axis=1)\n",
    "    return -np.mean(s)\n",
    "#    return np.mean(- np.sum(np.log(output) * (y_target), axis=0), axis=0)\n",
    "\n",
    "t_smax = np.array([[0.01, 0.04, 0.9],\n",
    "                  [0.01, 0.04, 0.9],\n",
    "                  ])\n",
    "t_y = np.array([[0,0,1],\n",
    "               [0,0,1]])\n",
    "xent = cross_entropy(smax, y_enc)\n",
    "print('Cross Entropy:', xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deepnotes.io/softmax-crossentropy\n",
    "#위를 따르면 크로스 엔트로피를 미분하면 결국 소프트맥스에서 y를 빼기만 하면 된다.\n",
    "# 소프트맥스와 y는 모두 클래스별 항목을 가지고 있다. 이 들을 빼면 클래스별로 값이 나온다.\n",
    "# 각 샘플별 평균을 내서 결국 클래스별 값을 구하는 것인가?\n",
    "# 중요한것! gradient의 결과는 W의 갯수와 같은 2X3이어야 한다.\n",
    "# X 항목 2개와 smax-y 3\n",
    "def gradient(smax, y_target):\n",
    "    #print(smax)\n",
    "#    return np.mean(smax-y_target ), axis=0)\n",
    "    #smax = y_target/smax\n",
    "    return np.dot(X.T, (smax-y_target) )/y_target.shape[0]\n",
    "grad = gradient(smax, y_enc)\n",
    "print('gradient', grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(X[y==0,0], X[y==0,1], 'r.')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 'g.')\n",
    "plt.plot(X[y==2,0], X[y==2,1], 'b.')\n",
    "'''\n",
    "W = np.random.rand(2,3)\n",
    "#W = np.ones((2,3))\n",
    "#W = np.array([[-0.5898155,0.10224475,0.48757075],\n",
    "#              [-0.57289249, 0.11754306, 0.45534943]])\n",
    "\n",
    "#bias = np.array([0.01, 0.1, 0.1])\n",
    "bias = np.random.rand(1,3)\n",
    "\n",
    "#learning_rate = 0.000001\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "costs = []\n",
    "for epoch in range(epochs):\n",
    "    #print('W:', W)\n",
    "    h = softmax(net_input(X, W))\n",
    "    #print(\"smax:\", h)\n",
    "    loss = cross_entropy(h, y_enc)\n",
    "    costs.append(loss)\n",
    "    #print('cost:', loss)\n",
    "    grad = gradient(h, y_enc)\n",
    "    #print('grad:', grad)\n",
    "    W = W - learning_rate * grad\n",
    "    if epoch %50 == 0 or epoch == 0:\n",
    "        print(epoch, loss)\n",
    "print(epoch, loss)\n",
    "\n",
    "\n",
    "mx = softmax(net_input(X, W))\n",
    "#print(mx)\n",
    "pred = to_classlabel(mx)\n",
    "print(pred)\n",
    "plt.plot(costs)\n",
    "plt.title(\"Loss\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array(['r', 'g', 'b'])\n",
    "plt.scatter(X[:,0], X[:,1], s=50, c=colormap[y])\n",
    "plt.title('Input', size=20)\n",
    "plt.show()\n",
    "plt.title('Predict', size=20)\n",
    "plt.scatter(X[:,0], X[:,1], s=50, c=colormap[pred])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Scikit-learn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "plt.plot(X[y==0,0], X[y==0,1], 'k^')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 'ko')\n",
    "plt.plot(X[y==2,0], X[y==2,1], 'ks')\n",
    "\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "softmax_reg.fit(X,y)\n",
    "print(softmax_reg.coef_)\n",
    "pred = softmax_reg.predict(X)\n",
    "print(pred)\n",
    "\n",
    "plt.plot(X[pred==0,0], X[pred==0,1], 'r.')\n",
    "plt.plot(X[pred==1,0], X[pred==1,1], 'g.')\n",
    "plt.plot(X[pred==2,0], X[pred==2,1], 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
