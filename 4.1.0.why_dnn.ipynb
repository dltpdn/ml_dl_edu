{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR 문제를 상수 W 값 MLP 예제\n",
    "* 2개 층으로 구성된 NN으로 XOR 문제를 푸는 예시\n",
    "* 단, W와 b를 이미 알고 있는 값을 지정해서 푼다.\n",
    "* 이것이 가능하다면 여러 개층의 W와 b를 GD로 풀수 있으면 많은 문제를 딥러닝으로 해결이 가능하다는 말이다.\n",
    "* 여기서 사용한 W와 b\n",
    "![xor_nn_1](https://user-images.githubusercontent.com/661959/54298177-9e82f080-45fb-11e9-8bdd-1f86718c6f5d.png)\n",
    "* $W$와 $bias$\n",
    "    * Layer-1 y1: $W = \\begin{bmatrix} 5 \\\\5 \\end{bmatrix} , b= -8$\n",
    "    * Layer-1 y2: $W = \\begin{bmatrix}-7 \\\\ -7 \\end{bmatrix} , b= 3$\n",
    "    * Layer-2 : $W= \\begin{bmatrix}-11 \\\\ -11 \\end{bmatrix}, b = 6$\n",
    "* 연산식\n",
    "    * $x=(0,0)$\n",
    "        * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 = -8, sig(-8)=0$\n",
    "        * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = 3, sig(3)=1$\n",
    "            * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =-11+6= -5, sig(-5)=0$\n",
    "    * $x=(0,1)$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5 -8=-3, sig(-3)=0$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7+3, sig(-4)=0$\n",
    "            * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =6, sig(6)=1$\n",
    "    * $x=(1,0)$\n",
    "        * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5 -8=-3, sig(-3)=0$\n",
    "        * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7+3, sig(-4)=0$\n",
    "            * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =6, sig(6)=1$\n",
    "    * $x=(1,1)$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5+5-8=2, sig(2)=1$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7-7+3=-11, sig(-11)=0$\n",
    "            * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =-11+6= -5, sig(-5)=0$\n",
    "* 연산 결과\n",
    "\n",
    "| $x_1$ $x_2$ | $y_1$ $y_2$ | $\\hat y$|\n",
    "|---|---|---|\n",
    "|0,0|0,1|0\n",
    "|0,1|0,0|1\n",
    "|1,0|0,0|1\n",
    "|1,1|1,0|0\n",
    "\n",
    "\n",
    "\n",
    "## Multi-variable 형식으로 축소\n",
    "![xor_nn_2](https://user-images.githubusercontent.com/661959/54298185-a347a480-45fb-11e9-91d4-e98111241794.png)\n",
    "\n",
    "* $W$와 bias\n",
    "    * $x = \\begin {bmatrix}0 & 0\\\\ 0&1\\\\1&0\\\\1&1 \\end {bmatrix}$\n",
    "    * $W_1 = \\begin{bmatrix}5&-7 \\\\ 5&-7\\end{bmatrix}$\n",
    "    * $b_1 = \\begin{bmatrix}-8 & 3 \\end{bmatrix}$\n",
    "    * $W_2 = \\begin{bmatrix}-11 \\\\ -11\\end{bmatrix}$\n",
    "    * $b_2 = 6 $\n",
    "* 연산식\n",
    "    * $ \\hat y =\n",
    "  \\begin{cases}\n",
    "    K(x) = sigmoid(Xw_1 + b_1)\\\\\n",
    "    H(x) = sigmoid(K(x)W_2 + b_2)\n",
    "  \\end{cases}\n",
    "    $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(np.array([[5,-7], [5,-7]], dtype=np.float32), name='weight1')\n",
    "b1 = tf.Variable(np.array([[-8, 3]], dtype=np.float32), name='bias1')\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(np.array([[-11],[-11]], dtype=np.float32), name='weight2')\n",
    "b2 = tf.Variable(np.array([6], dtype=np.float32), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   # print(sess.run(hypothesis , feed_dict={X: x_data, Y: y_data}))\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "* Chain Rule\n",
    "    * $f(g(x))$, $f(g)$, $g(x)$에 대해 미분\n",
    "        * $\\displaystyle \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x}$\n",
    "![](https://user-images.githubusercontent.com/661959/54297147-9924a680-45f9-11e9-9cab-9930330a8f19.png)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR 문제 MLP 학습 예제\n",
    "* 앞서 상수로 풀었던 XOR 문제를 학습해서 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  \n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Hypothesis:\n",
    "[[0.01338216]\n",
    " [0.98166394]\n",
    " [0.98809403]\n",
    " [0.01135799]] \n",
    "Predicted:\n",
    "[[0.]\n",
    " [1.]\n",
    " [1.]\n",
    " [0.]] \n",
    "Accuracy:\n",
    "1.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient\n",
    "![](https://t1.daumcdn.net/cfile/tistory/997E1B4C5BB6EAF239)\n",
    "* sigmoid 함수를 사용하면 input값들이(x1,x2,x3....xn) layer을 거치면서 0에 수렴\n",
    "* 0에 수렴하는 값들이 다른 layer의 input 값으로 입력된다.\n",
    "* 입력된 값들은 layer를 거치면서 0에 수렴\n",
    "* x1,x2,x3....xn의 값은 최종으로 출력 되는 값에 영향이 없다\n",
    "* cost가 줄어들지 않는다.\n",
    "\n",
    "## ReLU Activation Function \n",
    "* Rectified Linear Unit\n",
    "* `max(0,x)` \n",
    "\n",
    "# Weight 초기 값\n",
    "* 0을 사용하지 말것\n",
    "* RBM(Restricted Boltzmann Machine) 초창기 사용\n",
    "* Xavier\n",
    "    * `W = np.random.randn(in, out)/np.sqrt(in)`\n",
    "    * 입력값과 출력 값 사이의 난수를 입력 값의 제곱근으로 나눈다.\n",
    "* He\n",
    "    * `W = np.random.randn(in, out)/np.sqrt(in/2)`\n",
    "    * 입력 값을 2로 나눈 제곱근, 분모가 작아지기 때문에 xavier 보다 넓은 범위의 난수 \n",
    "    \n",
    "![](https://t1.daumcdn.net/cfile/tistory/2379CF4E57A0077534)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "ml_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
