{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2  Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 알고리즘\n",
    "* Regression toward the mean(Francis Galton, 1822~1911)\n",
    "* 샘플 데이타를 가장 잘 대표하는 직선의 방정식\n",
    "    * $y = ax + b$\n",
    "    * $a$ : 기울기\n",
    "    * $b$ : 절편\n",
    "* Hypothesis\n",
    "    * $H(x) = W_x + b$\n",
    "        * $H(x)$ : Hypothtesis\n",
    "        * $W$ : Weight\n",
    "        * $b$ : bias\n",
    " \n",
    "![image.png](https://i.imgur.com/ZMivez8.png)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression 개념 정리\n",
    "* Linear Regression을 Numpy 만으로 구현해 본다.\n",
    "* 단순화 하기 위해서 Weight 계수 1개만으로 제한 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플 데이타 생성\n",
    "* m개의 샘플 데이타를 만든다.\n",
    "* X 데이타 : np.arange(m) \n",
    "    * 0 ~ m\n",
    "* y 데이타 : X * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "m = 50 #sample count\n",
    "TRUE_W = 2\n",
    "\n",
    "X = tf.constant(range(m), tf.float32)\n",
    "y = X  * TRUE_W + 0 # W = 2, b=0(bias 생략)\n",
    "y = y + tf.random.normal((50,)) * 10 # Noise 추가 \n",
    "\n",
    "plt.plot(X, y, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형 회귀 표시\n",
    "* 데이타의 선회 회귀를 직선으로 표시\n",
    "* 최종적으로 이 선을 만족하는 W를 찾으면 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, X*TRUE_W, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis의 W의 변화와 Cost 의 관계\n",
    "* Hypothesis(가설)\n",
    "    * $H(W) = Wx^{(i)} + b$\n",
    "* Cost함수는 MSE(Mean Square Error, 평균 제곱 오차법)를 사용한다.\n",
    "    * $\\text{cost}(W) = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2$\n",
    "* W 값을 0.1 ~ 4.0까지 0.1씩 증가 하면서 cost를 구해서 시각화한다.\n",
    "    * $W$의 변화에 따라 Cost의 변화량을 시각화해 보면 MSE 함수는 Convex 함수임을 알 수 있다.\n",
    "\n",
    "### 관련 TF API\n",
    "* tf.sqaure() : 제곱\n",
    "    * tf.sqaure(3) : 9\n",
    "* tf.reduce_mean() :차원을 줄이면서 평균 계산\n",
    "    * tf.reduce_mean([1,2,3,4]) => 2.5\n",
    "* tf.reduce_sum() : 차원을 줄이면서 합 계산\n",
    "    * tf.reduce_sum([1,2,3,4,5]) : 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_range = tf.constant(np.arange(0, 4.1, 0.1), tf.float32) #[i/10 for i in range(0,41)]\n",
    "costs = []\n",
    "for i, w in enumerate(w_range):\n",
    "    h = w*X\n",
    "    #cost = 1/m * np.sum( (h - y)**2)\n",
    "    loss = h - y\n",
    "    sqr_err = tf.square(loss)\n",
    "    cost = tf.reduce_mean(sqr_err)\n",
    "    costs = np.append(costs, cost)\n",
    "    #plt.plot(w, cost, 'r.')\n",
    "plt.plot(w_range, costs, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost 함수와 Gradient\n",
    "\n",
    "* Cost 함수가 Convex 함수이기 때문에 최소값을 찾으려면 기울기를 따라 내려가면 된다.\n",
    "* 미분을 하면 기울기를 얻을 수 있다.\n",
    "    * $\\displaystyle\\frac{\\partial}{\\partial W}cost(W) = \\frac{2}{m}\\sum_{i=1}^m(H(W) -y^{(i)})x^{(i)}$\n",
    "* 미분한 결과를 Cost 함수의 변화에 시각화하면 다음과 같다.\n",
    "* 미분한 결과를 일정한 값(learning_rate)의 비율로 W에서 빼는 것을 반복하다 보면 cost가 가장 작은 W를 찾을 수 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_range, costs, 'ro')\n",
    "\n",
    "for w, cost in zip(w_range, costs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(w)\n",
    "        h = w*X\n",
    "        #gradient =  2/m * np.sum( (h-y) * X)\n",
    "        loss = h - y\n",
    "        sqr_err = tf.square(loss)\n",
    "        cost = tf.reduce_mean(sqr_err)\n",
    "        gradient = tape.gradient(cost, w)\n",
    "        #print(\"w:{},\\tcost:{},\\tgradient:{}\".format(w, cost, gradient))\n",
    "        #plt.plot(w_range, gradient *w_range + cost)\n",
    "        plt.plot(w_range, gradient*(w_range-w) +cost) #gradient는 x=0을 기준으로 계산되었기 때문에 \n",
    "        plt.axis([0, 4, -500, 3000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "* 전체 샘플에 대해서 경사 하강법을 이용해서 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, 'b.')\n",
    "\n",
    "W = tf.Variable(tf.random.normal(()))\n",
    "plt.plot(X, W*X, 'b', label=\"init H\")\n",
    "\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = W*X\n",
    "        cost = tf.reduce_mean(tf.square(h-Y))\n",
    "    grad = tape.gradient(cost, W)\n",
    "    W.assign(W - grad*0.0001)\n",
    "    plt.plot(X, W*X, '--')\n",
    "    if i%20 == 0:\n",
    "        print(f\"{i}, cost:{cost.numpy()}, W:{W.numpy()}\")\n",
    "plt.plot(X, W*X, 'r-', label=\"final H\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]))\n",
    "plt.plot(X, W*X, 'b', label='Initial H(x)')\n",
    "print('initial W', W.numpy())\n",
    "\n",
    "learning_rate = 0.00005\n",
    "suspend = 0.00001\n",
    "old_dW = None\n",
    "epochs = 1000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * X \n",
    "        loss = hypothesis - y\n",
    "        cost = tf.reduce_mean(tf.square(loss))\n",
    "        #cost = 1/m * np.sum(loss**2)\n",
    "        dW = tape.gradient(cost, W)\n",
    "        #dW=  2/m * np.sum(loss * X)\n",
    "        if old_dW is not None and abs(dW - old_dW) < suspend:\n",
    "            print('suspended:', epoch)\n",
    "            break\n",
    "        old_dW = dW\n",
    "        # W.assign(W - learning_rate * dW)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([dW], [W]))\n",
    "        #W.assign_sub(learning_rate * dW)\n",
    "        if epoch %20 == 0:\n",
    "            print('epoch:',epoch,' cost:',cost.numpy(),' dW:',gradient.numpy(), ' W:', W.numpy())\n",
    "            plt.plot(X, W*X, '--')\n",
    "print('Final W:', W.numpy())\n",
    "plt.plot(X, W*X, 'r', label='Final H(x)')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
