{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN(LSTM, GRU)\n",
    "* 시퀀스 데이터 처리할 수 있는 방법\n",
    "    * RNN\n",
    "    * 1D Convnet\n",
    "* 주요 사례\n",
    "    * 문서 분류, 시계열 분류\n",
    "    * 감성분석\n",
    "* 특징\n",
    "    * 이전 상태 유지\n",
    "    ```python\n",
    "    state = 0\n",
    "    for input in inputs:\n",
    "        outout, state = rnn_cell(input, state)\n",
    "    ```\n",
    "* RNN 개선\n",
    "    * LSTM(Long Short Term Memory)\n",
    "    * GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 데이터 다루기\n",
    "* 텍스트가 가장 흔한 시퀀스 데이타\n",
    "* 비전이 픽셀에 대한 패턴인식\n",
    "* 자연어처리는 단어, 문장, 문단에 대한 패턴인식\n",
    "* 입력데이타는 텍스트 원본일 수 없어서 텍스트 벡터화 필요\n",
    "    * 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환\n",
    "    * 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환\n",
    "    * 텍스트에서 단어나 문자의 n-그램을 추출하여 그것을 하나의 벡터로 변환\n",
    "        * n-gram: 문장에서 이웃한 N개의 문자\n",
    "        * n-gram은 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출\n",
    "            * \"The cat sat on the mat\"\n",
    "                * 2-gram : \"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"\n",
    "    * 어떻게 변환하든 이것을 토큰이라한다.\n",
    "    * 토큰에 수치형 벡터 연결\n",
    "        * 원핫 인코딩\n",
    "        * 토큰 임베딩\n",
    "\n",
    "### 단어와 문자의 원핫 인코딩\n",
    "* 토큰을 벡터로 변환하는 가장 일반적인 방법\n",
    "* 모든 단어에 고유한 인텍스 부여, 인텍스를 원핫 인코딩\n",
    "    * 단어가 아니라 문자를 대상으로 해도 됨\n",
    "* keras 원핫인코딩 유틸\n",
    "    * keras.preprocessing.text.Tokenizer\n",
    "    \n",
    "### 단어 임베딩 사용하기\n",
    "* 원핫 인코딩은 희소배열인데다가 단어나 문자수가 많아 지면 차원이 너무 높다.\n",
    "    * 밀집, 저차원이 필요\n",
    "        * 데이터로 학습\n",
    "            * 예)남성/여성, 복수/단수, 부정/긍정\n",
    "        * 사전 훈련 단어 임베딩 사용\n",
    "\n",
    "#### 훈련된 단어 임베딩 사용하기\n",
    "* Web3vec\n",
    "* GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 12 RNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
    "\n",
    "num_classes = 5\n",
    "input_dim = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.placeholder(\n",
    "    tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(\n",
    "    cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n",
    "# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n",
    "# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n",
    "outputs = tf.contrib.layers.fully_connected(\n",
    "    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM을 이용한 문장 자동 생성기\n",
    "* 박경리 토지 다운로드\n",
    "    * 국립국어원 : https://ithub.korean.go.kr/user/total/database/corpusView.do?boardSeq=2&articleSeq=2081&boardGb=T&isInsUpd=&boardType=CORPUS\n",
    "    * ./data/BEXX004.txt\n",
    "* 필요 라이브러리\n",
    "    * `!pip install beautifulsoup4`\n",
    "    * `!pip install konlpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random, sys\n",
    "\n",
    "fp = codecs.open(\"./data/BEXX0004.txt\", \"r\", encoding=\"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body\")\n",
    "text = body.getText() + \" \"\n",
    "print('코퍼스의 길이: ', len(text))\n",
    "# 문자를 하나하나 읽어 들이고 ID 붙이기\n",
    "chars = sorted(list(set(text)))\n",
    "print('사용되고 있는 문자의 수:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # 문자 → ID\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars)) # ID → 문자\n",
    "# 텍스트를 maxlen개의 문자로 자르고 다음에 오는 문자 등록하기\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('학습할 구문의 수:', len(sentences))\n",
    "print('텍스트를 ID 벡터로 변환합니다...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "# 모델 구축하기(LSTM)\n",
    "print('모델을 구축합니다...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "# 후보를 배열에서 꺼내기\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "# 학습시키고 텍스트 생성하기 반복\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('반복 =', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1) # \n",
    "    # 임의의 시작 텍스트 선택하기\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    # 다양한 다양성의 문장 생성\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('--- 다양성 = ', diversity)\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('--- 시드 = \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        # 시드를 기반으로 텍스트 자동 생성\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "            # 다음에 올 문자를 예측하기\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            # 출력하기\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "ml_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
